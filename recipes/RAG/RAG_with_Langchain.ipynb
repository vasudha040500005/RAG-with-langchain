{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vasudha040500005/RAG-and-AI/blob/main/recipes/RAG/RAG_with_Langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "s8WCB7GnSsT8"
      },
      "source": [
        "# Retrieval Augmented Generation (RAG) with Langchain\n",
        "*Using IBM Granite Models*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "HH5IH49KSsT9"
      },
      "source": [
        "## In this notebook\n",
        "This notebook contains instructions for performing Retrieval Augumented Generation (RAG). RAG is an architectural pattern that can be used to augment the performance of language models by recalling factual information from a knowledge base, and adding that information to the model query. The most common approach in RAG is to create dense vector representations of the knowledge base in order to retrieve text chunks that are semantically similar to a given user query.\n",
        "\n",
        "RAG use cases include:\n",
        "- Customer service: Answering questions about a product or service using facts from the product documentation.\n",
        "- Domain knowledge: Exploring a specialized domain (e.g., finance) using facts from papers or articles in the knowledge base.\n",
        "- News chat: Chatting about current events by calling up relevant recent news articles.\n",
        "\n",
        "In its simplest form, RAG requires 3 steps:\n",
        "\n",
        "- Initial setup:\n",
        "  - Index knowledge-base passages for efficient retrieval. In this recipe, we take embeddings of the passages, and store them in a vector database.\n",
        "- Upon each user query:\n",
        "  - Retrieve relevant passages from the database. In this recipe, we use an embedding of the query to retrieve semantically similar passages.\n",
        "  - Generate a response by feeding retrieved passage into a large language model, along with the user query."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "vpbE4lsSSsT-"
      },
      "source": [
        "## Setting up the environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bG6VFocESsT-"
      },
      "source": [
        "Install dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXQWtgPiSsT-",
        "outputId": "902f39e1-9db2-41d9-a300-0d4bdc902785"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "::group::Install Dependencies\n",
            "Collecting uv\n",
            "  Downloading uv-0.10.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Downloading uv-0.10.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (22.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.8/22.8 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: uv\n",
            "Successfully installed uv-0.10.0\n",
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m84 packages\u001b[0m \u001b[2min 13.14s\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mPrepared \u001b[1m18 packages\u001b[0m \u001b[2min 3.77s\u001b[0m\u001b[0m\n",
            "\u001b[2mUninstalled \u001b[1m3 packages\u001b[0m \u001b[2min 710ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m18 packages\u001b[0m \u001b[2min 215ms\u001b[0m\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mdataclasses-json\u001b[0m\u001b[2m==0.6.7\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mhuggingface-hub\u001b[0m\u001b[2m==1.3.7\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mhuggingface-hub\u001b[0m\u001b[2m==0.36.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mibm-granite-community-utils\u001b[0m\u001b[2m==0.1.dev134 (from git+https://github.com/ibm-granite-community/utils.git@984ad41bade71ba33a6e1f82f9285b8759ca2e76)\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mlangchain-classic\u001b[0m\u001b[2m==1.0.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mlangchain-community\u001b[0m\u001b[2m==0.4.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mlangchain-huggingface\u001b[0m\u001b[2m==1.2.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mlangchain-milvus\u001b[0m\u001b[2m==0.3.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mlangchain-replicate\u001b[0m\u001b[2m==0.1.dev26 (from git+https://github.com/ibm-granite-community/langchain-replicate.git@200c6f94a8c3bb59afc5dda0dfd88490cd5ba952)\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mlangchain-text-splitters\u001b[0m\u001b[2m==1.1.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmarshmallow\u001b[0m\u001b[2m==3.26.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmilvus-lite\u001b[0m\u001b[2m==2.5.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmypy-extensions\u001b[0m\u001b[2m==1.1.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpymilvus\u001b[0m\u001b[2m==2.6.8\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mreplicate\u001b[0m\u001b[2m==1.0.7\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mrequests\u001b[0m\u001b[2m==2.32.4\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mrequests\u001b[0m\u001b[2m==2.32.5\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==5.0.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==4.57.6\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtyping-inspect\u001b[0m\u001b[2m==0.9.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mwget\u001b[0m\u001b[2m==3.2\u001b[0m\n",
            "::endgroup::\n"
          ]
        }
      ],
      "source": [
        "! echo \"::group::Install Dependencies\"\n",
        "%pip install uv\n",
        "! uv pip install git+https://github.com/ibm-granite-community/utils.git \\\n",
        "    transformers \\\n",
        "    langchain_classic \\\n",
        "    langchain_community \\\n",
        "    langchain_text_splitters \\\n",
        "    langchain_huggingface sentence_transformers \\\n",
        "    langchain_milvus 'pymilvus[milvus_lite]' \\\n",
        "    'langchain_replicate @ git+https://github.com/ibm-granite-community/langchain-replicate.git' \\\n",
        "    wget\n",
        "! echo \"::endgroup::\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4isR0WDNSsT_"
      },
      "source": [
        "## Selecting System Components"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBNx0Q0XSsT_"
      },
      "source": [
        "### Choose your Embeddings Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61ymWPYeSsT_"
      },
      "source": [
        "Specify the model to use for generating embedding vectors from text.\n",
        "\n",
        "To use a model from a provider other than Huggingface, replace this code cell with one from [this Embeddings Model recipe](https://github.com/ibm-granite-community/granite-kitchen/blob/main/recipes/Components/Langchain_Embeddings_Models.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIjak6-5SsUA"
      },
      "outputs": [],
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "embeddings_model_path = \"ibm-granite/granite-embedding-30m-english\"\n",
        "embeddings_model = HuggingFaceEmbeddings(\n",
        "    model_name=embeddings_model_path,\n",
        ")\n",
        "embeddings_tokenizer = AutoTokenizer.from_pretrained(embeddings_model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cokRLTmUSsUA"
      },
      "source": [
        "### Choose your Vector Database\n",
        "\n",
        "Specify the database to use for storing and retrieving embedding vectors.\n",
        "\n",
        "To connect to a vector database other than Milvus substitute this code cell with one from [this Vector Store recipe](https://github.com/ibm-granite-community/granite-kitchen/blob/main/recipes/Components/Langchain_Vector_Stores.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWLrNY-1SsUB"
      },
      "outputs": [],
      "source": [
        "from langchain_milvus import Milvus\n",
        "import tempfile\n",
        "\n",
        "db_file = tempfile.NamedTemporaryFile(prefix=\"milvus_\", suffix=\".db\", delete=False).name\n",
        "print(f\"The vector database will be saved to {db_file}\")\n",
        "\n",
        "vector_db = Milvus(\n",
        "    embedding_function=embeddings_model,\n",
        "    connection_args={\"uri\": db_file},\n",
        "    auto_id=True,\n",
        "    index_params={\"index_type\": \"AUTOINDEX\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "ggGjgSUOSsUB"
      },
      "source": [
        "### Choose your LLM\n",
        "The LLM will be used for answering the question, given the retrieved text.\n",
        "\n",
        "Select a Granite Code model from the [`ibm-granite`](https://replicate.com/ibm-granite) org on Replicate. Here we use the Replicate Langchain client to connect to the model.\n",
        "\n",
        "To connect to a model on a provider other than Replicate, substitute this code cell with one from the [LLM component recipe](https://github.com/ibm-granite-community/granite-kitchen/blob/main/recipes/Components/Langchain_LLMs.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHsb_ymOSsUB"
      },
      "outputs": [],
      "source": [
        "from langchain_replicate import ChatReplicate\n",
        "from ibm_granite_community.notebook_utils import get_env_var\n",
        "\n",
        "model_path = \"ibm-granite/granite-4.0-h-small\"\n",
        "model = ChatReplicate(\n",
        "    model=model_path,\n",
        "    replicate_api_token=get_env_var('REPLICATE_API_TOKEN'),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "e4w2kzX2SsUB"
      },
      "source": [
        "## Building the Vector Database\n",
        "\n",
        "In this example, we take the State of the Union speech text, split it into chunks, derive embedding vectors using the embedding model, and load it into the vector database for querying."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vYRDjvqSsUB"
      },
      "source": [
        "### Download the document\n",
        "\n",
        "Here we use President Biden's State of the Union address from March 1, 2022."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "ZdC-BvxdSsUC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import wget\n",
        "\n",
        "filename = 'state_of_the_union.txt'\n",
        "url = 'https://raw.githubusercontent.com/IBM/watson-machine-learning-samples/master/cloud/data/foundation_models/state_of_the_union.txt'\n",
        "\n",
        "if not os.path.isfile(filename):\n",
        "  wget.download(url, out=filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YourOcZOSsUC"
      },
      "source": [
        "### Split the document into chunks\n",
        "\n",
        "Split the document into text segments that can fit into the model's context window."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "RD0vZXEaSsUC"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "\n",
        "loader = TextLoader(filename)\n",
        "documents = loader.load()\n",
        "text_splitter = CharacterTextSplitter.from_huggingface_tokenizer(\n",
        "    tokenizer=embeddings_tokenizer,\n",
        "    chunk_size=embeddings_tokenizer.max_len_single_sentence,\n",
        "    chunk_overlap=0,\n",
        ")\n",
        "texts = text_splitter.split_documents(documents)\n",
        "doc_id = 0\n",
        "for text in texts:\n",
        "    text.metadata[\"doc_id\"] = (doc_id:=doc_id+1)\n",
        "print(f\"{len(texts)} text document chunks created\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "gGrX1rF_SsUC"
      },
      "source": [
        "### Populate the vector database\n",
        "\n",
        "NOTE: Population of the vector database may take over a minute depending on your embedding model and service."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qKnEpXvMSsUD"
      },
      "outputs": [],
      "source": [
        "ids = vector_db.add_documents(texts)\n",
        "print(f\"{len(ids)} documents added to the vector database\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVo9XC2ESsUD"
      },
      "source": [
        "## Querying the Vector Database"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9orRsw3YSsUD"
      },
      "source": [
        "### Conduct a similarity search\n",
        "\n",
        "Search the database for similar documents by proximity of the embedded vector in vector space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1RoRugEASsUD"
      },
      "outputs": [],
      "source": [
        "query = \"What did the president say about Ketanji Brown Jackson?\"\n",
        "docs = vector_db.similarity_search(query)\n",
        "print(f\"{len(docs)} documents returned\")\n",
        "for doc in docs:\n",
        "    print(doc)\n",
        "    print(\"=\" * 80)  # Separator for clarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oyb0WutbSsUD"
      },
      "source": [
        "## Answering Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2s-17pNqSsUD"
      },
      "source": [
        "### Automate the RAG pipeline\n",
        "\n",
        "Build a RAG chain with the model and the document retriever.\n",
        "\n",
        "First we create the prompts for Granite to perform the RAG query. We use the Granite chat template and supply the placeholder values that the LangChain RAG pipeline will replace.\n",
        "\n",
        "Next, we construct the RAG pipeline by using the Granite prompt templates previously created."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNCUj_BUSsUD"
      },
      "outputs": [],
      "source": [
        "from ibm_granite_community.langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_classic.chains.retrieval import create_retrieval_chain\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# Create a Granite prompt for question-answering with the retrieved context\n",
        "prompt_template = ChatPromptTemplate.from_template(\"{input}\")\n",
        "\n",
        "# Assemble the retrieval-augmented generation chain\n",
        "combine_docs_chain = create_stuff_documents_chain(\n",
        "    llm=model,\n",
        "    prompt=prompt_template,\n",
        ")\n",
        "rag_chain = create_retrieval_chain(\n",
        "    retriever=vector_db.as_retriever(),\n",
        "    combine_docs_chain=combine_docs_chain,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "8WIrm6ucSsUD"
      },
      "source": [
        "### Generate a retrieval-augmented response to a question"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "U2Qkgp3QSsUD"
      },
      "source": [
        "Use the RAG chain to process a question. The document chunks relevant to that question are retrieved and used as context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "WyCG5LBoSsUE"
      },
      "outputs": [],
      "source": [
        "from ibm_granite_community.notebook_utils import wrap_text\n",
        "\n",
        "output = rag_chain.invoke({\"input\": query})\n",
        "\n",
        "print(wrap_text(output['answer']))"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}